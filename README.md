<h1 align="center">Rekhi</h1>


<p align="center">
  <a href="https://kaggle.com/seki32"><img src="https://img.shields.io/badge/Kaggle-seki32-20BEFF?style=flat&logo=kaggle&logoColor=white"/></a>&nbsp;
  <a href="https://github.com/Reiki"><img src="https://img.shields.io/badge/GitHub-Rekhii-181717?style=flat&logo=github&logoColor=white"/></a>&nbsp;
  <a href="https://linkedin.com/in/YOUR-LINKEDIN"><img src="https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=flat&logo=linkedin&logoColor=white"/></a>&nbsp;
  <a href="mailto:rekhi2607@gmail.com"><img src="https://img.shields.io/badge/Email-rekhi2607@gmail.com-D14836?style=flat&logo=gmail&logoColor=white"/></a>
</p>

---

### About

I code and learn every single day — no exceptions.
My approach: understand ML/DL from first principles before using frameworks. I derive the math, implement from scratch in NumPy, then scale with PyTorch. Theory without code is incomplete; code without theory is fragile.

Currently exploring LLM fine-tuning (LoRA, QLoRA, PEFT) on weekends while maintaining daily practice on classical ML and deep learning fundamentals.
I treat every dataset as an experiment and every model as a hypothesis to test. Consistency over intensity — small daily progress compounds.

---

### Projects

| Repository | Description | Status |
|:-----------|:------------|:------:|
| [**Data Every Day**](https://github.com/Rekhii/Data-Every-Day) | Daily practice — ML models (weekdays), LLM fine-tuning (weekends) | `ongoing` |
| [**Machine Learning Theory**](https://github.com/Rekhii/Machine-Learning-Theory) | Turning theory into code — ML algorithms from first principles | `ongoing` |
| [**Deep Learning Theory**](https://github.com/Rekhii/Deep-Learning-Theory) | Turning theory into code — neural networks from first principles | `ongoing` |
| [**Machine Learning**](https://github.com/Rekhii/Machine-Learning) | Core algorithms implemented in pure NumPy with math derivations | `ongoing` |
| [**Deep Learning**](https://github.com/Rekhii/Deep-Learning) | Neural networks from scratch — backprop, gradient flow | `ongoing` |
| [**Fine-Tuning LLM**](https://github.com/Rekhii/Fine-Tuning-LLM) | BERT, GPT, Qwen with LoRA/QLoRA | `ongoing` |

---

### Tools

| Category | Technologies |
|:---------|:-------------|
| **Core** | Python, NumPy, Pandas, scikit-learn |
| **Deep Learning** | PyTorch, TensorFlow |
| **LLMs** | Hugging Face, PEFT, TRL, LoRA, QLoRA |

---

### GitHub Stats



<p align="center">
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=Rekhii&hide_border=true&theme=graywhite" height="160"/>
</p>

<p align="center">
  <img src="https://github-readme-activity-graph.vercel.app/graph?username=Rekhii&bg_color=ffffff&color=000000&line=000000&point=ff0000&area=true&hide_border=true" height="200"/>
</p>

---

### Open to Work

Available for **internships**, **entry-level roles**, **research positions**, and **collaborations** in:

AI | ML | DL Engineering
